Conversation-First Voice Dating App
Technical & Implementation Specification (Next.js + Supabase + ElevenLabs)

1. Product overview
   A mobile-only dating app where:
   Users create a voice-based persona (agent) that represents them.
   Other users talk first to your agent (a voice replica) instead of swiping on photos.
   Matching is driven by:
   Length and quality of conversations with agents.
   Explicit interest (“like”) after talking.
   Once both people like each other (after sufficient conversation), they are matched and can connect directly (chat, calls, etc., in later versions).
   Core idea: match people based on how they talk, what they care about, and how they present themselves, not just الصور.
   Platform:
   Frontend: Next.js (React), mobile-only layout
   Backend: Supabase (auth, DB, edge functions)
   Voice & agents: ElevenLabs Agents Platform + Voice Cloning + React SDK
2. Target group
   Primary:
   Adults 18–40 who:
   Care about personality and communication.
   Prefer deeper conversation over fast swipe mechanics.
   Are open to speaking / listening via voice.
   Secondary:
   Busy professionals who want a “conversation-first” filter.
   Shy / introverted people who prefer talking via a mediated agent before “live” contact.
   People in regions where text-only apps feel too shallow or spammy.
   Key expectations:
   Simple onboarding, minimal friction.
   Strong privacy controls over voice cloning and data.
   Low latency, natural voice conversations.
3. Core user flows
   3.1 Onboarding (voice-first)
   Steps:
   Sign up / Auth
   Email + password, or OAuth (Google/Apple) via Supabase Auth.
   Enforce minimum age 18 with explicit confirmation.
   Basic profile
   Fields: name, age, gender.
   Optional: location (city/region), short text bio.
   Upload single profile photo (file upload to Supabase storage / S3).
   Onboarding voice agent conversation
   User enters a guided conversation with a dedicated onboarding agent (a system-wide ElevenLabs agent).
   Topics captured:
   What they are looking for in a partner (gender, age range, values).
   What they want to know about others (questions, topics, dealbreakers).
   What they can offer as a partner (strengths, interests, lifestyle).
   The app:
   Records structured answers (e.g., as JSON and tags).
   Optionally stores a free-form summary generated by the onboarding agent.
   Voice sample collection
   Prompt user to record several short clips (e.g. 30–60s total).
   Ask for explicit consent to clone their voice.
   Voice clone + personal agent creation
   Backend:
   Creates a voice clone with ElevenLabs using the uploaded audio.
   Creates a personal agent configured to:
   Use the cloned voice.
   Use a prompt derived from onboarding answers.
   Store resulting voice_id and agent_id in Supabase.
   Complete onboarding
   User lands in the agent feed screen once the agent is ready.
   If voice cloning is still processing, show a “Preparing your agent…” state and allow basic browsing with text-only agents.
   Mobile UX principles:
   Single-column, full-width layout.
   Big primary buttons.
   Large, clear recording controls.
   Use bottom-fixed CTA buttons and bottom sheets for modals.
   3.2 Agent feed and matching
   Feed
   Each feed card represents another user’s agent:
   Name (first name or alias).
   Age (or range).
   Basic tags (e.g. “Bookworm”, “Tech”, “Outdoors”).
   A short agent-written one-liner.
   “Talk to their agent” button.
   Starting a conversation
   Tap “Talk”:
   Use ElevenLabs React SDK to start a voice conversation with the target agent.
   Show full-screen call UI: waveform, timer, mic mute, end button.
   Conversation tracking
   Track:
   conversation_id (from ElevenLabs).
   Start / end timestamps.
   Duration in seconds.
   Simple rating: “Not for me” / “Good vibe” / “Amazing”.
   Store in Supabase conversations and conversation_events.
   Interest and match
   After a minimum duration (e.g. 3–5 minutes), show “Interested?” prompt.
   User can:
   Tap “Like” (store as pending like).
   Tap “Skip” (store as “not interested” and tune the feed).
   A match is created when:
   Both users have:
   Talked with each other’s agent for at least the minimum duration.
   Expressed interest (“Like”).
   Post-match
   Create a matches record.
   UI: Show match screen, then allow chat / scheduling call (later milestones).
4. ElevenLabs integration
   4.1 Basic setup
   Obtain ElevenLabs API key from dashboard (xi-api-key).
   Zuplo
   Store it as ELEVENLABS_API_KEY in Supabase / server environment.
   Never expose the key to the browser.
   Create a small backend client:
   // server/elevenlabsClient.ts
   const ELEVEN_API_BASE = 'https://api.elevenlabs.io';

export async function elevenFetch(path: string, init: RequestInit = {}) {
const headers = {
'xi-api-key': process.env.ELEVENLABS_API_KEY!,
...(init.headers || {}),
};

const res = await fetch(`${ELEVEN_API_BASE}${path}`, { ...init, headers });

if (!res.ok) {
throw new Error(`ElevenLabs error: ${res.status} ${await res.text()}`);
}

return res;
}
4.2 Create a new voice clone (Instant Voice Cloning)
From the docs, creating a voice clone is done via a POST request like:
ElevenLabs
POST https://api.elevenlabs.io/v1/voices/add
You send your audio sample(s) and metadata (name, description, etc.) along with the xi-api-key header.
Example Supabase Edge Function (simplified, no error handling):
// supabase/functions/create-voice-clone/index.ts
import { serve } from 'https://deno.land/std/http/server.ts';
import { elevenFetch } from '../\_shared/elevenlabsClient.ts';

serve(async (req) => {
// 1. Auth: verify Supabase user (omitted here)
const formData = await req.formData();

// formData should contain:
// - 'name'
// - 'files': audio file(s)

const elevenRes = await fetch(
'https://api.elevenlabs.io/v1/voices/add',
{
method: 'POST',
headers: {
'xi-api-key': Deno.env.get('ELEVENLABS_API_KEY')!,
},
body: formData, // forwarded as multipart/form-data
},
);

if (!elevenRes.ok) {
return new Response(await elevenRes.text(), { status: 500 });
}

const body = await elevenRes.json();

// body contains the new voice id (e.g. body.voice_id)
// Save `voice_id` into `voice_profiles` in Supabase (omitted here)

return new Response(JSON.stringify(body), {
headers: { 'Content-Type': 'application/json' },
});
});
Frontend flow:
User records audio in browser.
Upload audio to /api/create-voice-clone (Next API route calling the Edge function or directly hitting Supabase function).
Store returned voice_id in voice_profiles.
4.3 Create an Agent (Agents Platform)
Docs show the agent creation endpoint as:
11labs.ru
+1
POST https://api.elevenlabs.io/v1/convai/agents/create
Body: agent config (workflow & conversation behavior, tools, etc.)
The config typically includes:
Agent name.
Voice configuration (voice_id).
Prompt / system instructions (e.g., how to behave as the user’s dating agent).
First message.
Language, tools, knowledge base, etc.
Example server function:
// server/createAgent.ts
import { elevenFetch } from './elevenlabsClient';

interface CreateAgentInput {
name: string;
voiceId: string;
prompt: string;
firstMessage: string;
language?: string;
}

export async function createAgent(input: CreateAgentInput) {
const body = {
name: input.name,
agent: {
prompt: {
prompt: input.prompt,
},
firstMessage: input.firstMessage,
language: input.language ?? 'en',
},
tts: {
voiceId: input.voiceId,
},
// Additional config: tools, knowledge base, etc. as needed.
};

const res = await elevenFetch('/v1/convai/agents/create', {
method: 'POST',
headers: {
'Content-Type': 'application/json',
},
body: JSON.stringify(body),
});

return res.json(); // should include the new agent id
}
Usage:
After voice clone creation:
Build a prompt from onboarding answers (values, preferences, bio).
Call createAgent, store returned agent_id into agents table.
6.4 Using voice agents in React (Next.js, mobile)
ElevenLabs React SDK docs (React Agents Platform) give the basic usage:
ElevenLabs
Installation
npm install @elevenlabs/react

# or

yarn add @elevenlabs/react

# or

pnpm install @elevenlabs/react
Initialize a conversation hook
import { useConversation } from '@elevenlabs/react';

const conversation = useConversation();
The docs also emphasize requesting microphone access before starting a conversation:
ElevenLabs
await navigator.mediaDevices.getUserMedia({ audio: true });
Start a session with an agent
From the docs: startSession accepts agentId and connection settings:
ElevenLabs
const conversationId = await conversation.startSession({
agentId: '<agent-id>',
connectionType: 'webrtc', // or "websocket"
userId: '<your-end-user-id>', // optional
});
Minimal Next.js component for talking to an agent:
// app/conversation/[agentId]/page.tsx
'use client';

import { useParams } from 'next/navigation';
import { useEffect, useState } from 'react';
import { useConversation } from '@elevenlabs/react';

export default function ConversationPage() {
const { agentId } = useParams<{ agentId: string }>();
const [started, setStarted] = useState(false);
const conversation = useConversation({
onError: (e) => console.error(e),
onStatusChange: (status) => console.log('status', status),
});

useEffect(() => {
if (!agentId || started) return;

    const start = async () => {
      await navigator.mediaDevices.getUserMedia({ audio: true });

      await conversation.startSession({
        agentId,
        connectionType: 'webrtc',
        userId: 'current-user-id', // from auth
      });

      setStarted(true);
    };

    start().catch(console.error);

}, [agentId, started, conversation]);

return (

<main className="min-h-screen flex flex-col items-center justify-center p-4">
<h1 className="text-xl font-semibold mb-4">Talking to agent</h1>
{/_ Add mic mute / end buttons, waveform, timer, etc. _/}
<button
className="mt-4 rounded-full px-6 py-3 border"
onClick={() => conversation.endSession()} >
End
</button>
</main>
);
}
Use this both for:
Onboarding conversations (with a shared onboarding agent ID).
Talking to other users’ personal agents (agent IDs from agents table).
